{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model Per Recipe 2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"PykwpIdeYxVT","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install catboost\n","!pip install tsfresh\n","!pip install xgboost\n","!pip install shap"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_n8lC0MjY0RG","colab_type":"code","colab":{}},"cell_type":"code","source":["# The essentials\n","import pandas as pd\n","import numpy as np\n","\n","from collections import defaultdict\n","\n","# Plotting\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","# Progress bars\n","from tqdm import tqdm\n","\n","# Access our Google Drive\n","from google.colab import drive\n","\n","# Gradient Boosting\n","from catboost import CatBoostRegressor, Pool\n","from xgboost import XGBRegressor\n","\n","# TSFRESH Feature Extraction\n","from tsfresh import extract_features\n","from tsfresh.feature_extraction import EfficientFCParameters\n","from tsfresh.utilities.dataframe_functions import impute\n","from tsfresh.feature_selection.relevance import calculate_relevance_table\n","\n","from sklearn.model_selection import KFold, GridSearchCV\n","from sklearn.linear_model import Lasso, Ridge\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn.pipeline import Pipeline\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n","\n","from collections import defaultdict, Counter\n","from scipy.stats import norm\n","\n","from scipy.stats import boxcox, boxcox_normmax\n","from scipy.special import inv_boxcox\n","\n","from sklearn.preprocessing import PowerTransformer, StandardScaler\n","\n","import shap"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1WnhPRYJY10w","colab_type":"code","colab":{}},"cell_type":"code","source":["drive.mount('/content/drive', force_remount=True)\n","!ls \"/content/drive/My Drive/Rinse Over Run\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"56z_ZLa_Y25C","colab_type":"code","colab":{}},"cell_type":"code","source":["train_df = pd.read_csv('/content/drive/My Drive/Rinse Over Run/train_values.csv', index_col=0, parse_dates=['timestamp'])\n","test_df = pd.read_csv('/content/drive/My Drive/Rinse Over Run/test_values.csv', index_col=0, parse_dates=['timestamp'])\n","label_df = pd.read_csv('/content/drive/My Drive/Rinse Over Run/train_labels.csv', index_col='process_id')\n","all_data = pd.concat([train_df, test_df], axis=0)\n","\n","train_df = train_df[train_df['phase'] != 'final_rinse']\n","\n","train_df['phase_int'] = train_df['phase'].map({'pre_rinse': 1, \n","                                               'caustic': 2, \n","                                               'intermediate_rinse': 4, \n","                                               'acid': 8})\n","test_df['phase_int'] = test_df['phase'].map({'pre_rinse': 1, \n","                                             'caustic': 2, \n","                                             'intermediate_rinse': 4, \n","                                             'acid': 8})\n","train_process_combinations = pd.DataFrame(train_df.groupby('process_id')['phase_int'].unique().apply(lambda x: sum(x)))\n","test_process_combinations = pd.DataFrame(test_df.groupby('process_id')['phase_int'].unique().apply(lambda x: sum(x)))\n","process_combinations = pd.concat([train_process_combinations, test_process_combinations], axis=0)\n","\n","recipe_df = pd.read_csv('/content/drive/My Drive/Rinse Over Run/recipe_metadata.csv', index_col='process_id')\n","recipe_df = recipe_df.drop('final_rinse', axis=1)\n","recipe_df['pre_rinse_num'] = recipe_df['pre_rinse'] * 1\n","recipe_df['caustic_num'] = recipe_df['caustic'] * 2\n","recipe_df['intermediate_rinse_num'] = recipe_df['intermediate_rinse'] * 4\n","recipe_df['acid_num'] = recipe_df['acid'] * 8\n","recipe_df['recipe'] = recipe_df['pre_rinse_num'] + recipe_df['caustic_num'] + recipe_df['intermediate_rinse_num'] + recipe_df['acid_num']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vf96p4BXY6MU","colab_type":"code","colab":{}},"cell_type":"code","source":["ts_real = [\n","    'supply_flow',\n","    'supply_pressure',\n","    'return_temperature',\n","    'return_conductivity',\n","    'return_turbidity',\n","    'return_flow',\n","    'tank_level_pre_rinse',\n","    'tank_level_caustic',\n","    'tank_level_acid',\n","    'tank_level_clean_water',\n","    'tank_temperature_pre_rinse',\n","    'tank_temperature_caustic',\n","    'tank_temperature_acid',\n","    'tank_concentration_caustic',\n","    'tank_concentration_acid',\n","    'target_value',\n","    'flow_diff',\n","    'supply_flow_log',\n","    'return_flow_log'\n","]\n","\n","# variables we'll use to create our time series features\n","ts_cols = [\n","    'supply_flow',\n","    'supply_pressure',\n","    'return_temperature',\n","    'return_conductivity',\n","    'return_turbidity',\n","    'return_flow',\n","    'tank_level_pre_rinse',\n","    'tank_level_caustic',\n","    'tank_level_acid',\n","    'tank_level_clean_water',\n","    'tank_temperature_pre_rinse',\n","    'tank_temperature_caustic',\n","    'tank_temperature_acid',\n","    'tank_concentration_caustic',\n","    'tank_concentration_acid',\n","    'target_value',\n","    'flow_diff',\n","    #'supply_flow_log',\n","    #'return_flow_log'\n","]\n","\n","# variables for binary time series features\n","bin_cols = [\n","    'supply_pump',\n","    'supply_pre_rinse',\n","    'supply_caustic',\n","    'return_caustic',\n","    'supply_acid',\n","    'return_acid',\n","    'supply_clean_water',\n","    'return_recovery_water',\n","    'return_drain',\n","    'object_low_level',\n","    'tank_lsh_caustic',\n","    'tank_lsh_acid',\n","    'tank_lsh_clean_water',\n","    'tank_lsh_pre_rinse'\n","]\n","\n","process_comb_to_phases = {\n","    15: ['pre_rinse', 'caustic', 'intermediate_rinse', 'acid'],\n","    3:  ['pre_rinse', 'caustic'],\n","    7:  ['pre_rinse', 'caustic', 'intermediate_rinse'],\n","    1:  ['pre_rinse'],\n","    8:  ['acid'],\n","    2:  ['caustic'],\n","    6:  ['caustic', 'intermediate_rinse'],\n","    14: ['caustic', 'intermediate_rinse', 'acid'],\n","}\n","\n","# phases, ordered from earliest to latest\n","phases = ['pre_rinse', 'caustic', 'intermediate_rinse', 'acid']\n","\n","def encode_categorical(df):\n","    # Currently just copy-pasted from http://drivendata.co/blog/rinse-over-run-benchmark/\n","    \n","    # select process_id and pipeline\n","    meta = df[['process_id', 'pipeline', 'object_id']].drop_duplicates().set_index('process_id') \n","    meta['object_id'] = meta['object_id'] // 10\n","    \n","    # convert categorical pipeline data to dummy variables\n","    meta = pd.get_dummies(meta, columns=['pipeline', 'object_id'])\n","    \n","    # pipeline L12 not in test data (so useless feature)\n","    if 'pipeline_L12' in meta:\n","        meta = meta.drop('pipeline_L12', axis=1)\n","    \n","    return meta\n","  \n","def count_zeros(x):\n","  return np.sum(x == 0)\n","  \n","def encode_real_timeseries(df):   \n","    ts_df = df[['process_id'] + ts_cols].set_index('process_id')\n","    \n","    # create features: count, min, max, mean, standard deviation\n","    ts_features = ts_df.groupby('process_id').agg(['min', 'max', 'mean', 'std', \n","                                                   'count', 'median', 'sum', \n","                                                   lambda x: x.tail(5).mean(),\n","                                                   count_zeros])\n","    \n","    cols = []\n","    for col in ts_features.columns:\n","        cols.append('real_{}'.format(col))\n","    ts_features.columns = cols\n","    \n","    return ts_features\n","\n","def encode_binary_timeseries(df):\n","    ts_df = df[['process_id'] + bin_cols].set_index('process_id')\n","            \n","    # create features: count, min, max, mean, standard deviation\n","    ts_features = ts_df.groupby('process_id').agg(['mean', 'std', \n","                                                   lambda x: x.tail(5).mean(),\n","                                                   count_zeros])\n","    \n","    cols = []\n","    for col in ts_features.columns:\n","        cols.append('bin_{}'.format(col))\n","    ts_features.columns = cols\n","    \n","    return ts_features\n","  \n","def get_tsfresh_features(df):\n","    extraction_settings = EfficientFCParameters()\n","    filtered_funcs = ['abs_energy', 'mean_abs_change', 'mean_change', \n","                      'skewness', 'kurtosis', 'absolute_sum_of_changes', \n","                      'longest_strike_below_mean', 'longest_strike_above_mean', \n","                      'count_above_mean', 'count_below_mean', 'last_location_of_maximum', \n","                      'first_location_of_maximum', 'last_location_of_minimum', \n","                      'first_location_of_minimum', \n","                      'percentage_of_reoccurring_datapoints_to_all_datapoints', \n","                      'percentage_of_reoccurring_values_to_all_values', \n","                      'sum_of_reoccurring_values', 'sum_of_reoccurring_data_points', \n","                      'ratio_value_number_to_time_series_length', 'maximum', 'minimum', \n","                      'cid_ce', 'symmetry_looking', 'large_standard_deviation', 'quantile', \n","                      'autocorrelation', 'number_peaks', 'binned_entropy', 'index_mass_quantile', \n","                      'linear_trend',  'number_crossing_m']\n","#     new_funcs = ['augmented_dickey_fuller', 'number_cwt_peaks', 'agg_autocorrelation',\n","#                'spkt_welch_density', 'friedrich_coefficients', 'max_langevin_fixed_point',\n","#                'c3', 'ar_coefficient', 'mean_second_derivative_central', 'ratio_beyond_r_sigma',\n","#                'energy_ratio_by_chunks', 'partial_autocorrelation',\n","#                'fft_aggregated', 'time_reversal_asymmetry_statistic', 'range_count']\n","#     filtered_funcs += new_funcs\n","    filtered_settings = {}\n","    for func in filtered_funcs:\n","      filtered_settings[func] = extraction_settings[func]\n","\n","    ts_features = extract_features(df[['process_id', 'timestamp', 'return_turbidity', 'return_flow', 'supply_flow', 'target_value', 'flow_diff']], \n","                                   column_id='process_id', column_sort=\"timestamp\", \n","                                   column_kind=None, column_value=None,\n","                                   impute_function=impute, \n","                                   default_fc_parameters=filtered_settings,\n","                                   show_warnings=False)\n","  \n","    return ts_features\n","                                       \n","\n","def create_feature_matrix(df, processes, phases):\n","    df['return_flow'] = df['return_flow'].apply(lambda x: max(x, 0))\n","    df['supply_flow'] = df['supply_flow'].apply(lambda x: max(x, 0))\n","    df['target_value'] = df['return_flow'] * df['return_turbidity']\n","    df['flow_diff'] = df['supply_flow'] - df['return_flow']\n","    \n","    phase_data = df[(df['process_id'].isin(processes)) &\n","                    ((df['phase'].isin(phases)))]\n","    \n","    metadata = encode_categorical(phase_data)\n","    time_series = encode_real_timeseries(phase_data)\n","    binary_features = encode_binary_timeseries(phase_data)\n","    \n","    if len(phases) > 1:\n","      last_phase_data = phase_data[phase_data['phase'] == phases[-1]]\n","      time_series_last_phase = encode_real_timeseries(last_phase_data)\n","      new_cols = []\n","      for col in time_series_last_phase.columns:\n","        new_cols.append('last_{}'.format(col))\n","      time_series_last_phase.columns = new_cols\n","      binary_features_last_phase = encode_binary_timeseries(last_phase_data)\n","      new_cols = []\n","      for col in binary_features_last_phase.columns:\n","        new_cols.append('last_{}'.format(col))\n","      binary_features_last_phase.columns = new_cols\n","    \n","    tsfresh_features = get_tsfresh_features(phase_data)\n","    \n","    # join metadata and time series features into a single dataframe\n","    feature_matrix = metadata\n","    feature_matrix = feature_matrix.merge(time_series, left_index=True, right_index=True)\n","    feature_matrix = feature_matrix.merge(binary_features, left_index=True, right_index=True)\n","    feature_matrix = feature_matrix.merge(tsfresh_features, left_index=True, right_index=True)\n","    \n","    if len(phases) > 1:\n","      feature_matrix = feature_matrix.merge(time_series_last_phase, left_index=True, right_index=True)\n","      feature_matrix = feature_matrix.merge(binary_features_last_phase, left_index=True, right_index=True)\n","    \n","    return feature_matrix\n","    \n","  \n","def get_processes(data, phases, train=True):\n","    filtered_processes = []\n","    phases = set(phases)\n","    processes = set(data['process_id'])\n","    for process in processes:\n","        process_phases = set(data[data['process_id'] == process]['phase'])\n","        if train:\n","            if phases.issubset(process_phases):\n","                filtered_processes.append(process)\n","        else:\n","            if len(phases) == len(process_phases) == len(phases.intersection(process_phases)):\n","                filtered_processes.append(process)\n","    return filtered_processes"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BF6ZZjKiZKs1","colab_type":"code","colab":{}},"cell_type":"code","source":["def custom_mape(approxes, targets):\n","    return np.mean(np.abs(np.subtract(approxes, targets)) / np.maximum(np.abs(targets), 290000))\n","\n","class MAPEMetric(object):\n","    def get_final_error(self, error, weight):\n","        return error\n","\n","    def is_max_optimal(self):\n","        return False\n","\n","    def evaluate(self, approxes, targets, weight):\n","        return custom_mape(np.exp(approxes), np.exp(targets)), len(targets)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uNVRvwSVZNuh","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.base import clone\n","\n","def fit_stack(clf, name, X_train, y_train, X_test, n_splits=5):\n","  scaler = StandardScaler()\n","  cols = X_train.columns\n","  train_idx = X_train.index\n","  test_idx = X_test.index\n","  X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=cols, index=train_idx)\n","  X_test = pd.DataFrame(scaler.transform(X_test), columns=cols, index=test_idx)\n","  \n","  train_predictions = np.zeros((len(X_train),))\n","  test_predictions = np.zeros((len(X_test), n_splits))\n","  kf = KFold(n_splits=n_splits, shuffle=True)\n","  for fold_ix, (train_idx, test_idx) in enumerate(kf.split(X_train, y_train)):\n","    X_cv_train = X_train.iloc[train_idx, :]\n","    X_cv_test = X_train.iloc[test_idx, :]\n","    y_cv_train = y_train.iloc[train_idx]\n","    y_cv_test = y_train.iloc[test_idx]\n","    \n","    clf_clone = clone(clf)\n","    clf_clone.fit(X_cv_train, y_cv_train)\n","    \n","    print('[{}] Fold #{} MAPE={}'.format(name, fold_ix + 1, custom_mape(np.exp(y_cv_test), np.exp(clf_clone.predict(X_cv_test)))))\n","    \n","    train_predictions[test_idx] = np.minimum(np.max(y_cv_train), np.maximum(0, clf_clone.predict(X_cv_test)))\n","    test_predictions[:, fold_ix] = np.minimum(np.max(y_cv_train), np.maximum(0, clf_clone.predict(X_test)))\n","    \n","  train_predictions_df = pd.DataFrame(train_predictions, index=X_train.index, columns=['{}_pred'.format(name)])\n","  # Taking min instead of mean, since undershooting is better than overshooting for MAPE\n","  test_predictions_df = pd.DataFrame(np.min(test_predictions, axis=1), index=X_test.index, columns=['{}_pred'.format(name)])\n","    \n","  return train_predictions_df, test_predictions_df"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JFVfL7eqZO6Q","colab_type":"code","colab":{}},"cell_type":"code","source":["#from tsfresh.feature_selection.relevance import calculate_relevance_table\n","\n","def get_corr_features(X):\n","  row_idx, col_idx = np.where(X.corr() == 1)\n","  self_corr = set([(i, i) for i in range(X.shape[1])])\n","  return set(list(zip(row_idx, col_idx))) - self_corr \n","\n","def get_uncorr_features(data):\n","  X_train_corr = data.copy()\n","  correlated_features = get_corr_features(X_train_corr)\n","  \n","  corr_cols = set()\n","  for row_idx, col_idx in correlated_features:\n","    corr_cols.add(row_idx)\n","    corr_cols.add(col_idx)\n","  \n","  uncorr_cols = list(set(X_train_corr.columns) - set(X_train_corr.columns[list(corr_cols)]))\n","   \n","  col_mask = [False]*X_train_corr.shape[1]\n","  for col in corr_cols:\n","    col_mask[col] = True\n","  X_train_corr = X_train_corr.loc[:, col_mask]\n","  \n","  correlated_features = get_corr_features(X_train_corr)\n","  \n","  while correlated_features:\n","    print('{} correlated feature pairs left...'.format(len(correlated_features)))\n","    corr_row, corr_col = correlated_features.pop()\n","    col_mask = [True]*X_train_corr.shape[1]\n","    col_mask[corr_row] = False\n","    X_train_corr = X_train_corr.loc[:, col_mask]\n","    correlated_features = get_corr_features(X_train_corr)\n","  return list(set(list(X_train_corr.columns) + uncorr_cols))\n","\n","def remove_features(data, target, p_val=0.25):\n","  single_cols = list(data.columns[data.nunique() == 1])\n","  \n","  uncorr_cols = get_uncorr_features(data)\n","  corr_cols = list(set(data.columns) - set(uncorr_cols))\n","  \n","  return list(set(single_cols + corr_cols))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RJgosGqFZUv-","colab_type":"code","colab":{}},"cell_type":"code","source":["combinations_per_recipe = {\n","    3: [3], \n","    9: [8],\n","    15: [1]\n","}\n","\n","import warnings; warnings.filterwarnings('ignore')\n","\n","prediction_df = None\n","all_mapes = defaultdict(list)\n","for recipe in [15]:\n","    recipe_train_data = train_df[train_df['process_id'].isin(recipe_df[recipe_df['recipe'] == recipe].index)]\n","    recipe_test_data = test_df[test_df['process_id'].isin(recipe_df[recipe_df['recipe'] == recipe].index)]\n","    for process_combination in combinations_per_recipe[recipe]:\n","      print('Recipe = {} || Combination = {}'.format(recipe, process_combination))\n","      train_processes = get_processes(recipe_train_data, process_comb_to_phases[process_combination])\n","      phase_features = create_feature_matrix(train_df, train_processes, process_comb_to_phases[process_combination])\n","      \n","      X = phase_features.loc[train_processes]\n","      y = np.log(label_df.loc[X.index]['final_rinse_total_turbidity_liter'])\n","    \n","      to_drop = remove_features(X, y)\n","      X = X.drop(to_drop, axis=1)\n","    \n","      kf = KFold(n_splits=5, random_state=2019)\n","      mapes = []\n","      shaps = []\n","      for train_idx, test_idx in kf.split(X, y):\n","        X_train = X.iloc[train_idx, :]\n","        X_test = X.iloc[test_idx, :]\n","\n","        y_train = y.iloc[train_idx]\n","        y_test = y.iloc[test_idx]\n","        \n","        clfs = [\n","            ('knn', GridSearchCV(KNeighborsRegressor(), {'n_neighbors': [3, 5, 10, 25, 100]})),\n","            ('lr', GridSearchCV(Lasso(max_iter=10000), {'alpha': [1.0, 10.0, 100.0, 1000.0]})),\n","            ('knn_pca', GridSearchCV(\n","                Pipeline(steps=[('pca', PCA()), ('knn', KNeighborsRegressor())]),\n","                {'knn__n_neighbors': [10, 25, 100], 'pca__n_components': [5, 10, 25]}\n","              )\n","            ),\n","            ('mlp', GridSearchCV(MLPRegressor(max_iter=1000), {'hidden_layer_sizes': [(100,), (250,), (100, 100)]})),\n","            ('rf_25', RandomForestRegressor(n_estimators=25)),\n","            ('rf_100', RandomForestRegressor(n_estimators=100)),\n","            ('rf_250', RandomForestRegressor(n_estimators=250)),\n","            ('et_25', ExtraTreesRegressor(n_estimators=25)),\n","            ('et_100', ExtraTreesRegressor(n_estimators=100)),\n","            ('et_250', ExtraTreesRegressor(n_estimators=250)),\n","        ]\n","        \n","        for name, clf in clfs:\n","            train_pred_df, test_pred_df = fit_stack(clf, name, X_train, y_train, X_test)\n","            X_train = pd.concat([X_train, train_pred_df], axis=1)\n","            X_test = pd.concat([X_test, test_pred_df], axis=1)\n","            \n","        train_idx = np.random.choice(X_train.index, replace=False, size=int(0.9 * len(X_train)))\n","        val_idx = list(set(X_train.index) - set(train_idx))\n","\n","        X_val = X_train.loc[val_idx, :]\n","        y_val = y_train.loc[val_idx]\n","        X_train = X_train.loc[train_idx, :]\n","        y_train = y_train.loc[train_idx]\n","    \n","        if (recipe, process_combination) in [(9, 8)]:\n","          recipe_15_train_data = train_df[train_df['process_id'].isin(recipe_df[recipe_df['recipe'] == 15].index)]\n","            \n","          extra_processes = get_processes(recipe_15_train_data, process_comb_to_phases[process_combination])\n","          extra_phase_data = train_df[(train_df['process_id'].isin(extra_processes)) &\n","                                      ((train_df['phase'].isin(process_comb_to_phases[process_combination])))]\n","          \n","          extra_phase_features = create_feature_matrix(train_df, extra_processes, process_comb_to_phases[process_combination])\n","          X_extra = extra_phase_features.loc[list(set(extra_phase_data['process_id']))]\n","          # WARNING: This does not work!!!! Use log-transform when we augment\n","          y_extra = np.log(label_df.loc[X_extra.index]['final_rinse_total_turbidity_liter'])\n","\n","          for col in set(X_train.columns) - set(X_extra.columns):\n","              X_extra[col] = 0\n","          X_extra = X_extra[X.columns]\n","\n","          X_train = pd.concat([X_train, X_extra])\n","          y_train = pd.concat([y_train, y_extra])\n","        \n","        \n","        print(list(X_train.columns))\n","        \n","        print('CV TRAIN = {} || CV VAL = {} || CV TEST = {}'.format(X_train.shape, X_val.shape, X_test.shape))\n","\n","        cat = CatBoostRegressor(iterations=100000, od_type='Iter', od_wait=100, \n","                                learning_rate=0.33,\n","                                loss_function='MAPE', eval_metric='MAPE', task_type='GPU')\n","        cat.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=50)\n","        \n","        explainer = shap.TreeExplainer(cat)\n","        shap_values = explainer.shap_values(pd.concat([X_train, X_val, X_test]))\n","        \n","        plt.figure()\n","        shap.summary_plot(shap_values, pd.concat([X_train, X_val, X_test]), max_display=30, \n","                          auto_size_plot=True, show=False, color_bar=False)\n","        plt.show()\n","        \n","        predictions = np.exp(cat.predict(X_test))\n","        mape = custom_mape(predictions, np.exp(y_test))\n","        print('TEST MAPE = {}'.format(mape))\n","        mapes.append(mape)\n","        all_mapes[(recipe, process_combination)].append(mape)\n","\n","      print('Combination = {}, MAPE = {}+-{}'.format(process_combination, np.mean(mapes), np.std(mapes)))\n","      \n","    print('Recipe {}: MAPES: {}'.format(recipe, all_mapes))\n","    \n","for k in all_mapes:\n","    print(k, np.mean(all_mapes[k]), np.std(all_mapes[k]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"N0tswOlJZhDb","colab_type":"code","colab":{}},"cell_type":"code","source":["combinations_per_recipe = {\n","    3: [3], #1, 2, \n","    9: [8],\n","    15: [1, 3, 7, 15] # 2, 6, 14\n","}\n","\n","prediction_df = None\n","import warnings; warnings.filterwarnings('ignore')\n","for recipe in [3]:\n","    recipe_train_data = train_df[train_df['process_id'].isin(recipe_df[recipe_df['recipe'] == recipe].index)]\n","    recipe_test_data = test_df[test_df['process_id'].isin(recipe_df[recipe_df['recipe'] == recipe].index)]\n","    for process_combination in combinations_per_recipe[recipe]:\n","      print('Recipe = {} || Combination = {}'.format(recipe, process_combination))\n","      train_processes = get_processes(recipe_train_data, process_comb_to_phases[process_combination])\n","      test_processes = get_processes(recipe_test_data, process_comb_to_phases[process_combination], train=False)\n","      all_processes = train_processes + test_processes\n","\n","      phase_features = create_feature_matrix(all_data, all_processes, process_comb_to_phases[process_combination])\n","\n","      X_train = phase_features.loc[train_processes]\n","      X_test = phase_features.loc[test_processes]\n","\n","      y_train = np.log(label_df.loc[X_train.index]['final_rinse_total_turbidity_liter'])\n","      \n","      to_drop = remove_features(X_train, y_train)\n","      print(len(to_drop), to_drop)\n","\n","      X_train = X_train.drop(to_drop, axis=1)\n","      X_test = X_test.drop(to_drop, axis=1)\n","      \n","      if (recipe, process_combination) in [(9, 8)]:\n","          if recipe == 9:\n","            recipe_15_train_data = train_df[train_df['process_id'].isin(recipe_df[recipe_df['recipe'] == 15].index)]\n","          else:\n","            recipe_15_train_data = train_df[train_df['process_id'].isin(recipe_df[recipe_df['recipe'] == 3].index)]\n","            \n","          extra_processes = get_processes(recipe_15_train_data, process_comb_to_phases[process_combination])\n","          extra_phase_data = train_df[(train_df['process_id'].isin(extra_processes)) &\n","                                      ((train_df['phase'].isin(process_comb_to_phases[process_combination])))]\n","          extra_phase_features = create_feature_matrix(all_data, extra_processes, process_comb_to_phases[process_combination])\n","          X_extra = extra_phase_features.loc[list(set(extra_phase_data['process_id']))]\n","          y_extra = np.log(label_df.loc[X_extra.index]['final_rinse_total_turbidity_liter'])\n","          \n","          for col in set(X_train.columns) - set(X_extra.columns):\n","              X_extra[col] = 0\n","          X_extra = X_extra[X_train.columns]\n","\n","          X_train = pd.concat([X_train, X_extra])\n","          y_train = pd.concat([y_train, y_extra])\n","        \n","      clfs = [\n","          ('knn', GridSearchCV(KNeighborsRegressor(), {'n_neighbors': [3, 5, 10, 25, 100]})),\n","          ('lr', GridSearchCV(Lasso(max_iter=10000), {'alpha': [1.0, 10.0, 100.0, 1000.0]})),\n","          ('knn_pca', GridSearchCV(\n","              Pipeline(steps=[('pca', PCA()), ('knn', KNeighborsRegressor())]),\n","              {'knn__n_neighbors': [10, 25, 100], 'pca__n_components': [5, 10, 25]}\n","            )\n","          ),\n","          ('mlp', GridSearchCV(MLPRegressor(max_iter=1000), {'hidden_layer_sizes': [(100,), (250,), (100, 100)]})),\n","          ('rf_25', RandomForestRegressor(n_estimators=25)),\n","          ('rf_100', RandomForestRegressor(n_estimators=100)),\n","          ('rf_250', RandomForestRegressor(n_estimators=250)),\n","          ('et_25', ExtraTreesRegressor(n_estimators=25)),\n","          ('et_100', ExtraTreesRegressor(n_estimators=100)),\n","          ('et_250', ExtraTreesRegressor(n_estimators=250)),\n","          #('tsne_knn', Pipeline(steps=[('tsne', TSNE(n_components=2)), ('knn', GridSearchCV(KNeighborsRegressor(), {'n_neighbors': [3, 5, 10, 25, 100]}))]))\n","      ]\n","\n","      for name, clf in clfs:\n","          train_pred_df, test_pred_df = fit_stack(clf, name, X_train, y_train, X_test, n_splits=5)\n","          X_train = pd.concat([X_train, train_pred_df], axis=1)\n","          X_test = pd.concat([X_test, test_pred_df], axis=1)\n","\n","      train_idx = np.random.choice(X_train.index, replace=False, size=int(0.9 * len(X_train)))\n","      val_idx = list(set(X_train.index) - set(train_idx))\n","\n","      X_val = X_train.loc[val_idx, :]\n","      X_train = X_train.loc[train_idx, :]\n","      y_val = y_train.loc[val_idx]\n","      y_train = y_train.loc[train_idx]\n","      \n","      print(X_train.shape, X_val.shape, X_test.shape)\n","\n","      cat = CatBoostRegressor(iterations=100000, od_type='Iter', od_wait=100, learning_rate=0.33,\n","                              loss_function='MAPE', eval_metric='MAPE', task_type='GPU')\n","      cat.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=50)\n","\n","      predictions = np.exp(cat.predict(X_test))\n","\n","      sub_predictions_df = pd.DataFrame(predictions, columns=['final_rinse_total_turbidity_liter'])\n","      sub_predictions_df.index = X_test.index\n","      sub_predictions_df.index.name = X_test.index.name\n","\n","      if prediction_df is None:\n","          prediction_df = sub_predictions_df\n","      else:\n","          prediction_df = pd.concat([prediction_df, sub_predictions_df])\n","\n","      del cat"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LVvKr-C6ZjWq","colab_type":"code","colab":{}},"cell_type":"code","source":["prediction_df = prediction_df.sort_index()\n","prediction_df.index.name = X_test.index.name\n","prediction_df = prediction_df.sort_index()\n","prediction_df.to_csv('/content/drive/My Drive/Rinse Over Run/stacking_v2_not_all_models.csv')\n","\n","print(len(prediction_df))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4YaBGV8JZnKk","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}